{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-27e1cd2a4b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_dictionary_annotations_and_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0038'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-27e1cd2a4b19>\u001b[0m in \u001b[0;36mmake_dictionary_annotations_and_text\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mannotation_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'additional'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mannotation_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.items())# list_norm_annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "# Read a file and split into lines\n",
    "def read_norm_lines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [line.split('||') for line in lines]\n",
    "\n",
    "def make_dictionary_annotations_and_text(name):\n",
    "    text_file_path = str(name) + \".txt\"\n",
    "    text_norm_path = str(name) + \".norm\"\n",
    "    note_text = open(os.path.join('/Users/isabelmetzger/PycharmProjects/ClinicalScorecard/data/train/train_note', text_file_path)).read()\n",
    "    list_norm_annotations = read_norm_lines(os.path.join('/Users/isabelmetzger/PycharmProjects/ClinicalScorecard/data/train/train_norm', text_norm_path))\n",
    "    annotation_dictionary = {'id': [], 'concept': [], 'beginCharacterOffset': [],\n",
    "                             'endCharacterOffset': [], 'text': [], 'additional': {'beginCharacterOffset': [],\n",
    "                                                                                  'endCharacterOffset': [],\n",
    "                                                                                  'text': []}}\n",
    "    for list_norm in list_norm_annotations:\n",
    "        annotation_dictionary['id'].append(list_norm[0])\n",
    "        annotation_dictionary['concept'].append(list_norm[1])\n",
    "        annotation_dictionary['beginCharacterOffset'].append(list_norm[2])\n",
    "        annotation_dictionary['endCharacterOffset'].append(list_norm[3])\n",
    "        annotated_text = note_text[int(list_norm[2]): int(list_norm[3])]\n",
    "        annotation_dictionary['text'].append(annotated_text)\n",
    "        if len(list_norm) > 5:\n",
    "            annotation_dictionary['additional']['beginCharacterOffset'].append(list_norm[4])\n",
    "            annotation_dictionary['additional']['endCharacterOffset'].append(list_norm[5])\n",
    "            annotation_dictionary['additional']['text'].append(note_text[int(list_norm[4]): int(list_norm[5])])\n",
    "            \n",
    "    return annotation_dictionary #.items())# list_norm_annotations\n",
    "\n",
    "\n",
    "pprint.pprint(make_dictionary_annotations_and_text('0038'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import unicodedata\n",
    "\n",
    "def findFiles(path):\n",
    "    \"\"\"\n",
    "    returns a list of files in a path\n",
    "    \"\"\"\n",
    "    return glob.glob(path)\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeStringAndDigitsLower(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.:!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[\\|]+\", r\" \", s)\n",
    "    s = re.sub('\\n', '<NEWLINE>', s)\n",
    "    s = re.sub('\\t', ' ', s)\n",
    "    s = re.sub('\\d+', '#', s)\n",
    "    s = re.sub(' +', ' ',s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_vocab(vocab_min, infile, vocab_filename):\n",
    "    \"\"\"\n",
    "        INPUTS:\n",
    "            vocab_min: how many documents a word must appear in to be kept\n",
    "            infile: (training) data file to build vocabulary from\n",
    "            vocab_filename: name for the file to output\n",
    "    \"\"\"\n",
    "    with open(infile, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        #header\n",
    "        next(reader)\n",
    "\n",
    "        #0. read in data\n",
    "        print(\"reading in data...\")\n",
    "        #holds number of terms in each document\n",
    "        note_numwords = []\n",
    "        #indices where notes start\n",
    "        note_inds = [0]\n",
    "        #indices of discovered words\n",
    "        indices = []\n",
    "        #holds a bunch of ones\n",
    "        data = []\n",
    "        #keep track of discovered words\n",
    "        vocab = {}\n",
    "        #build lookup table for terms\n",
    "        num2term = {}\n",
    "        #preallocate array to hold number of notes each term appears in\n",
    "        note_occur = np.zeros(400000, dtype=int)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            text = row[2]\n",
    "            numwords = 0\n",
    "            for term in text.split():\n",
    "                #put term in vocab if it's not there. else, get the index\n",
    "                index = vocab.setdefault(term, len(vocab))\n",
    "                indices.append(index)\n",
    "                num2term[index] = term\n",
    "                data.append(1)\n",
    "                numwords += 1\n",
    "            #record where the next note starts\n",
    "            note_inds.append(len(indices))\n",
    "            indset = set(indices[note_inds[-2]:note_inds[-1]])\n",
    "            #go thru all the word indices you just added, and add to the note occurrence count for each of them\n",
    "            for ind in indset:\n",
    "                note_occur[ind] += 1\n",
    "            note_numwords.append(numwords)\n",
    "            i += 1\n",
    "        #clip trailing zeros\n",
    "        note_occur = note_occur[note_occur>0]\n",
    "\n",
    "        #turn vocab into a list so indexing doesn't get fd up when we drop rows\n",
    "        vocab_list = np.array([word for word,ind in sorted(vocab.items(), key=operator.itemgetter(1))])\n",
    "\n",
    "        #1. create sparse document matrix\n",
    "        C = csr_matrix((data, indices, note_inds), dtype=int).transpose()\n",
    "        #also need the numwords array to be a sparse matrix\n",
    "        note_numwords = csr_matrix(1. / np.array(note_numwords))\n",
    "        \n",
    "        #2. remove rows with less than 3 total occurrences\n",
    "        print(\"removing rare terms\")\n",
    "        #inds holds indices of rows corresponding to terms that occur in < 3 documents\n",
    "        inds = np.nonzero(note_occur >= vocab_min)[0]\n",
    "        print(str(len(inds)) + \" terms qualify out of \" + str(C.shape[0]) + \" total\")\n",
    "        #drop those rows\n",
    "        C = C[inds,:]\n",
    "        note_occur = note_occur[inds]\n",
    "        vocab_list = vocab_list[inds]\n",
    "\n",
    "        print(\"writing output\")\n",
    "        with open(vocab_filename, 'w') as vocab_file:\n",
    "            for word in vocab_list:\n",
    "                vocab_file.write(word + \"\\n\")\n",
    "\n",
    "\n",
    "def letterToIndex(letter):\n",
    "    \"\"\"\n",
    "    find index for all letters\n",
    "    \"\"\"\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def letterToTensor(letter):\n",
    "    \"\"\"\n",
    "    Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "    \"\"\"\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def lineToTensor(line):\n",
    "    \"\"\"# Turn a line into a <line_length x 1 x n_letters>, or an array of one-hot letter vectors\"\"\"\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "all_CNER_words = {}\n",
    "all_CNER_labels = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
